{
  "name": "Yelp Reviews Recommendation",
  "tagline": "EECS 349 Machine Learning project",
  "body": "### Members\r\n\r\n_Listed in alphabetically order_\r\n\r\nDesheng Liu (dli848): deshengliu2015@u.northwestern.edu\r\n\r\nShiping Zhao (szd041): shipingzhao2015@u.northwestern.edu\r\n\r\nYan Liu (ylk070): yanliu2015@u.northwestern.edu\r\n\r\n### Organization\r\n\r\nThis is a Machine Learning project in EECS 349 at Northwestern University.\r\n\r\n### Synopsis\r\n\r\nThe reviews we would see in the Yelp app in a first glance are mainly 2 kinds: reviews with high votes, and reviews that are fresh. Although the reviews with high votes could be useful, they are probably out-of-date. Thus, fresh reviews are also needed to be displayed in the reviews flow. However, most of the fresh reviews are low quality, or say, useless. It would be better to rank those more useful reviews to be higher than less useful ones, so that the users could read the useful ones first to save their time.\r\n\r\nTo rank the reviews in terms of their quality (weighted by kinds of votes and normalized by post time), the learners we tried to use are regression learners. To be more specific, we used Linear Regression, Ridge Regression, Passive Aggressive Regression, AdaBoost, Random Forest, etc. In terms of features, we used both text features (bag of words) and metadata of reviews and their users. In addition, the dimentsion of the text features are normalized by overall frequency and reduced using PCA technique.\r\n\r\nThe dataset we used are the json files achieved from [Yelp's official website](https://www.yelp.com/dataset_challenge). We first preprocessed them to be standardized csv files. Then trained and tested them with scikit-learn library. To measure the success, we used the Dummy Regressor learner as the baseline. It is similar to the ZeroR classifier in Weka which simply predicts the result by assigning a simple value (i.e. mean, median, constant quality). We compared the performaces of different kinds of regression learners with the baseline learner.\r\n\r\nWe evaluated the performance of regression learners with R2 score. It could be negative because the prediction for regression problem could be arbitrarily bad. And the closer to 1, the better it is. For the result, the R2 score of the __baseline learner is -9.22 * 10^-7__. The __best learner we find is Random Forest, earning 0.48 R2 score__. The second best ones are Linear and Ridge Regression, both getting 0.26 R2 score. The key features that greatly improved our result are metadata of reviews and users, such as votes, stars, etc. When using only text features, we only get 0.02 R2 score with Linear Regression and 0.03 with Ridge Regression.\r\n\r\n### Performance\r\n\r\nFeatures: Bag of Words\r\n\r\n| Models             | Mean Absolute Error | R2           |\r\n| -----------------  | ------------------- | ------------ |\r\n| Dummy Regression   | 1.35                | -0.008       |\r\n| Linear Regression  | 1.32                | 0.02         |\r\n| Ridge Regression   | 1.28                | 0.03         |\r\n| Passive Aggressive | 1.36                | -0.08        |\r\n| AdaBoost           | 4.74                | -3.71        |\r\n| Random Forest      | running             | running      |\r\n\r\nFeatures: Original Features + Bag of Words\r\n\r\n| Models             | Mean Absolute Error | R2           |\r\n| -----------------  | ------------------- | ------------ |\r\n| Dummy Regression   | 1.27                | -9.22 * e^-7 |\r\n| Linear Regression  | 1.07                | 0.26         |\r\n| Ridge Regression   | 1.07                | 0.26         |\r\n| Passive Aggressive | 1.42                | -2.10        |\r\n| AdaBoost           | 15.17               | -23.69       |\r\n| Random Forest      | 0.98                | 0.48         |\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}